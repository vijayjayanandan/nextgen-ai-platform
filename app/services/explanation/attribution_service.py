from typing import Dict, List, Optional, Any, Set, Tuple
import re
import uuid
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.core.config import settings
from app.core.logging import get_logger
from app.models.document import Document, DocumentChunk

logger = get_logger(__name__)


class AttributionService:
    """
    Service for providing attributions and explanations for AI-generated content
    by linking it to source documents.
    """
    
    def __init__(
        self,
        db: AsyncSession
    ):
        """
        Initialize the attribution service.
        
        Args:
            db: Database session
        """
        self.db = db
    
    async def process_attributions(
        self,
        generated_text: str,
        source_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Process attributions for generated text based on source chunks.
        
        Args:
            generated_text: Text generated by the LLM
            source_chunks: Source document chunks used for generation
            
        Returns:
            Dictionary with attribution information
        """
        # If explainability is disabled, return empty attributions
        if not settings.ENABLE_EXPLANATION:
            return {
                "text": generated_text,
                "attributions": [],
                "has_attributions": False
            }
        
        # Get document information for the chunks
        document_ids = set()
        for chunk in source_chunks:
            if "document_id" in chunk:
                document_ids.add(chunk["document_id"])
                
        # Fetch document info
        documents = {}
        if document_ids:
            for doc_id in document_ids:
                try:
                    uuid_id = uuid.UUID(doc_id)
                    result = await self.db.execute(select(Document).where(Document.id == uuid_id))
                    document = result.scalars().first()
                    if document:
                        documents[doc_id] = {
                            "id": str(document.id),
                            "title": document.title,
                            "source_type": document.source_type.value,
                            "source_id": document.source_id,
                            "source_url": document.source_url,
                            "content_type": document.content_type,
                            "security_classification": document.security_classification
                        }
                except ValueError:
                    # Invalid UUID, skip
                    continue
                except Exception as e:
                    logger.error(f"Error fetching document {doc_id}: {str(e)}")
        
        # Simple attribution based on keyword matching
        attributions = []
        
        # Process each source chunk
        for i, chunk in enumerate(source_chunks):
            chunk_content = chunk.get("content", "")
            document_id = chunk.get("document_id")
            
            if not chunk_content or not document_id:
                continue
                
            # Simple attribution approach: look for significant phrases from the chunk in the generated text
            # In a production system, this would use more sophisticated techniques
            
            # Extract phrases (simplistic approach)
            significant_phrases = self._extract_significant_phrases(chunk_content)
            
            # Look for these phrases in the generated text
            for phrase in significant_phrases:
                if len(phrase) > 7 and phrase.lower() in generated_text.lower():
                    # Found a match
                    doc_info = documents.get(document_id, {"id": document_id})
                    
                    attributions.append({
                        "text": phrase,
                        "chunk_id": chunk.get("id", f"chunk_{i}"),
                        "document_id": document_id,
                        "document_title": doc_info.get("title", "Unknown Document"),
                        "source_type": doc_info.get("source_type", "unknown"),
                        "page_number": chunk.get("page_number"),
                        "section_title": chunk.get("section_title")
                    })
        
        # De-duplicate attributions
        unique_attributions = []
        seen_phrases = set()
        
        for attr in attributions:
            if attr["text"] not in seen_phrases:
                seen_phrases.add(attr["text"])
                unique_attributions.append(attr)
        
        return {
            "text": generated_text,
            "attributions": unique_attributions,
            "has_attributions": len(unique_attributions) > 0,
            "documents": [doc for doc in documents.values()]
        }
    
    async def add_citations(
        self,
        generated_text: str,
        source_chunks: List[Dict[str, Any]]
    ) -> str:
        """
        Add citations to generated text based on source chunks.
        
        Args:
            generated_text: Text generated by the LLM
            source_chunks: Source document chunks used for generation
            
        Returns:
            Text with citations added
        """
        # If explainability is disabled, return original text
        if not settings.ENABLE_EXPLANATION:
            return generated_text
        
        # Get attribution info
        attribution_info = await self.process_attributions(generated_text, source_chunks)
        
        # If no attributions, return original text
        if not attribution_info["has_attributions"]:
            return generated_text
        
        # Add footnote citations
        cited_text = generated_text
        citations = []
        
        # Sort attributions by length (longest first) to avoid nested citations
        sorted_attributions = sorted(
            attribution_info["attributions"], 
            key=lambda x: len(x["text"]), 
            reverse=True
        )
        
        # Replace phrases with citations
        for i, attr in enumerate(sorted_attributions):
            phrase = attr["text"]
            citation_number = i + 1
            
            # Replace phrase with cited version (case-insensitive)
            pattern = re.escape(phrase)
            cited_phrase = f"{phrase}[{citation_number}]"
            cited_text = re.sub(pattern, cited_phrase, cited_text, flags=re.IGNORECASE)
            
            # Build citation text
            doc_title = attr.get("document_title", "Unknown Document")
            citation = f"[{citation_number}] {doc_title}"
            
            if attr.get("page_number"):
                citation += f", page {attr['page_number']}"
                
            if attr.get("section_title"):
                citation += f", section '{attr['section_title']}'"
                
            citations.append(citation)
        
        # Add citations as footnotes
        if citations:
            cited_text += "\n\nSources:\n"
            cited_text += "\n".join(citations)
        
        return cited_text
    
    def _extract_significant_phrases(self, text: str, min_length: int = 5) -> List[str]:
        """
        Extract significant phrases from text.
        
        Args:
            text: Source text
            min_length: Minimum word length for phrases
            
        Returns:
            List of significant phrases
        """
        # Simple approach: extract sentences and phrases
        # In a production system, use NLP to extract key phrases
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # Extract shorter phrases from sentences
        phrases = []
        
        for sentence in sentences:
            if len(sentence.split()) >= min_length:
                phrases.append(sentence.strip())
                
            # Also extract sub-phrases
            if len(sentence.split()) > min_length * 2:
                # Split on commas and other punctuation
                sub_phrases = re.split(r'[,;:]', sentence)
                for phrase in sub_phrases:
                    if len(phrase.split()) >= min_length:
                        phrases.append(phrase.strip())
        
        return phrases
    
    async def get_source_documents(
        self,
        chunk_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Get source document information for a list of chunk IDs.
        
        Args:
            chunk_ids: List of chunk IDs
            
        Returns:
            List of document information dictionaries
        """
        # Convert string IDs to UUIDs
        uuid_ids = []
        for id_str in chunk_ids:
            try:
                uuid_ids.append(uuid.UUID(id_str))
            except ValueError:
                # Invalid UUID, skip
                continue
        
        if not uuid_ids:
            return []
            
        # Fetch chunks
        result = await self.db.execute(
            select(DocumentChunk).where(DocumentChunk.id.in_(uuid_ids))
        )
        chunks = result.scalars().all()
        
        # Get document IDs
        document_ids = set(chunk.document_id for chunk in chunks)
        
        # Fetch documents
        documents = []
        for doc_id in document_ids:
            result = await self.db.execute(select(Document).where(Document.id == doc_id))
            document = result.scalars().first()
            
            if document:
                documents.append({
                    "id": str(document.id),
                    "title": document.title,
                    "description": document.description,
                    "source_type": document.source_type.value,
                    "source_id": document.source_id,
                    "source_url": document.source_url,
                    "content_type": document.content_type,
                    "language": document.language,
                    "security_classification": document.security_classification,
                    "chunks": [
                        {
                            "id": str(chunk.id),
                            "content": chunk.content,
                            "chunk_index": chunk.chunk_index,
                            "page_number": chunk.page_number,
                            "section_title": chunk.section_title
                        }
                        for chunk in chunks
                        if chunk.document_id == doc_id
                    ]
                })
        
        return documents